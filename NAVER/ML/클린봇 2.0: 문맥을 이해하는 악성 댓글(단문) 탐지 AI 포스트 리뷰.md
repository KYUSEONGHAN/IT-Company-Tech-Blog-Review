![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/23/Naver_Logotype.svg/2560px-Naver_Logotype.svg.png)


![](https://d2.naver.com/content/images/2020/07/f704d100-b70c-11ea-8501-0d897b3eccd4.png)

## 목차
1. 악플이란 무엇인가
2. 데이터셋 구축
3. 모델링
4. 학습
5. 전이 학습

## 1. 악플이란 무엇인가
- 욕설: 일반적인 욕설, 네이버 내부적으로 가지고 있는 욕설 데이터에 포함된 표현
- 저속한 표현: 타인에게 불쾌감을 주는 속되고, 격이 낮은 표현
- 선정적인 표현: 성적으로 자극적인 표현
- 폭력적인 표현: 신체적 위협에 대한 표현
- 차별적인 표현: 지역/인종/국가/종교 등에 기반한 차별 표현
- 비하적인 표현: 상대방에게 모멸감과 수치심을 주는 비하 표현

## 2. 데이터셋 구축
- 뉴스의 댓글 데이터를 샘플링화하며 각 댓글의 악플 여부를 판별해 레이블링 작업 수행
- 레이블링 규칙은 아래와 같다.
    - 정의된 유형의 표현을 포함: 2로 레이블링
    - 판단하기 애매함: 1로 레이블링
    - 정의된 유형의 표현을 포함하지 않음: 0으로 레이블링
- 위 규칙에 따른 데이터셋 확보 결과 1점은 악플 여부 판단에 어려워 데이터셋에서 제외했고, 2점을 부여받은 댓글은 악플, 0점이면 악플이 아닌걸로 간주
- 총 35만여 건의 레이블링된 댓글 데이터셋을 구축.

## 3. 모델링
- 입출력 정의
    - 입력: 댓글 내용
    - 출력: 0 ~ 1 사이의 실수 -> 댓글 내용이 악플일 확률
- 입력 데이터 전처리
    - 모델에 댓글을 입력 시켜 결과를 얻기 위해 댓글을 토큰화해야 한다.
    - 댓글에는 특수문자를 활용한 표현이 자주 나타나므로 단어나 형태소 단위는 적절하지 않다.
    - 따라서 음절 단위로 댓글을 토큰화 수행
- 손실 함수 정의
    - 모델 학습을 위한 손실 함수(loss function)은 다음과 같이 정의했다.
    - 이는 binary cross entropy 함수에 해당한다.
    ![](https://d2.naver.com/content/images/2020/07/b1fc6580-b579-11ea-95e2-71a2f74218e4.png)
- 모델 구조 설계
    - BERT와 같은 무거운 SOTA 모델을 사용하기보다는 비교적 가벼운 수준의 규모와 연산 복잡도를 유지하는 선에서 뉴럴 넷 구조를 설계하려고 했다.
    - 그 이유는 아래와 같다.
        - 정의된 태스크가 단순하다.(이진 분류)
        - 모델 파라미터의 증가보다는 데이터의 충원에 의한 성능 향상 효과가 훨씬 뚜렷했다.
        - 고정된 데이터에서 최고의 성능을 끌어올리는 것보다 빠른 댓글 트렌드 변화에 따라 지속적으로 업데이트를 하는 것이 악플 문제에 더 효과적이다. 무거운 모델일수록 업데이트를 위한 시간과 계산 비용이 매우 늘어나는 문제점이 있다.
        - 댓글 작성에 대한 사용자 경험이 저해되지 않기 위해서 최소의 응답 속도와 서비스 안정성을 보장해야 한다.
    - 다음과 같은 뉴럴 넷 구조를 가지고 비교 실험을 진행했다.
        - CNN
            - Embedding Layer
            - 2 One-Dimension Convolution Layers (Dropout: 50%)
            - 1-Dimension Global Max Pooling
            - Concatenate
            - Fully Connected Layer
            - 1-Dimension Fully Connected Layer
            - Sigmoid Activation
        - BiLSTM
            - Embedding Layer
            - Bidirectional LSTM (Dropout: 10%)
            - 1-Dimension Global Max Pooling
            - 1-Dimension Fully Connected Layer
            - Sigmoid Activation
        - BiLSTM + LSTM
            - Embedding Layer
            - Bidirectional LSTM + Residual Connection from Embedding Layer
            - Forward LSTM
            - 1-Dimsension Global Max Pooling
            - 1-Dimension Fully Connected Layer (Dropout: 50%)
            - Sigmoid Activation
        - CNN + BiLSTM + LSTM
            - Embedding Layer
            - 1-Dimension Convolution Layer (Dropout: 50%)
            - Bidirectional LSTM + Residual Connection from Embedding Layer
            - Forward LSTM
            - 1-Dimsension Global Max Pooling
            - 1-Dimension Fully Connected Layer (Dropout: 50%)
            - Sigmoid Activation
    - CNN이란
        - convolutional neural network
        - 시각적 이미지 분석에 가장 일반적으로 사용되는 기계 학습의 세부 방법론 중 하나로, 자연어처리에 사용된다
    - BiLSTM이란
        - Bidirectional LSTM
        - 게이트 기법을 통해 순환 신경 회로망(RNN)의 한계를 극복한 모델인 LSTM을 순방향뿐 아니라 역방향의 결과를 함께 이용하는 모델이다. 문맥(Context)을 기반으로 하는 연관성 분석에 유리하여 속도를 고려한 다양한 NLP 문제에 널리 활용되고 있다.
        
## 4. 학습
![](https://d2.naver.com/content/images/2020/07/0b55c780-b727-11ea-9b7e-df2d7578dbfa.png)
- 오버샘플링
    - 일반적으로 분류 모델을 학습시킬 때, 각 분류에 해당하는 데이터가 불균형할 경우 학습 시 편향이 발생할 가능성이 있다.
    - 구축한 학습 데이터의 악플과 악플이 아닌 댓갈 간의 비율이 1:2 정도였기 때문에, 악플에 해당하는 뎅터를 2배 정도로 복제해 균형을 맞추었다.
- 모델에 의한 데이터 정제
    - 레이블링은 사람에 의한 작업이기 때문에 얼마든지 오류가 발생할 수 있다.
    - 학습 데이터에서 명백히 악플이 아님에도 불구하고, 그 반대로 잘못 레이블링한 사례를 찾을 수 있었다.
    - 이러한 노이즈를 제거하기 위해 모델을 활용한 노이즈 정제 방법을 사용하면서 아래 절차를 반복하면 노이즈 정게 과정이 완료된다.
        1. 데이터를 일정한 사이즈의 chunk로 N 등분한다.
        2. n번째 등분의 chunk를 테스트 데이터로, 나머지 모든 chunk의 합을 트레이닝 데이터로 삼고 모델을 학습시킨다.
        3. 학습된 모델을 이용해 테스트 데이터로 삼았던 n번째 등분의 데이터에 대해 예측 결과를 산출한다.
        4. 예측 결과 스코어값(악플일 확률)이 높으면서 악플이 아닌 것으로 레이블링되어 있거나 예측 결과 스코어값이 낮으면서 악플인 것으로 레이블링되어 있는 데이터를 우선으로 검수해 레이블 오류가 있는지 확인/교정한다.
    
## 5. 전이 학습
- Transfer Learning
- 목표로 하는 태스크에 대해 모델을 학습하기 전에 관련성 있는 다른 테스크를 학습하는 과정을 먼저 거치게 함으로써 모델의 성능 향상을 꾀하는 방법이다.
- 전이 학습 방법론으로 작성자 동일성 여부를 기반으로 댓글 간의 유사도를 산출하는 모델 학습 방법과 ELMO 태스크를 수행하는 방법 두 가지를 시도했다.
- Persona Embedding, 댓글 유사도 학습
    - 소개하는 댓글 유사도 학습 방법은 이용자피드백플랫폼에서 자체적으로 고안한 방법론으로, 중복 댓글 탐지 모델의 학습 방법에 ㅐㅎ당한다.
    - 텍스트의 유사도를 판단하는 과정에는 두 가지 측명이 있다고 간주한다.
        - 두 텍스트를 구성하고 있는 어휘가 얼마나 중복되는가라는 정량적인 측명 -> 코사인 유사도 사용
        - 두 텍스트가 얼마나 의미적으로 유사한가라는 정성적인 측면
    - 두 가지 측면을 종합하면 유사도를 평가하는모델을 학습시킬 수 있다.
![](https://d2.naver.com/content/images/2020/07/bd955100-b4d3-11ea-9ed9-2bb74f33878c.png)
- ELMO
    - 다층의 LSTM 레이어의 출력 결괄르 취합해 언어 모델 태스크를 수행하는 방법으로 모델을 학습시킨다.
![](https://d2.naver.com/content/images/2020/07/a191af80-b4d3-11ea-901a-9acc8eb9d626.png)


## 참고자료 출처
[https://d2.naver.com/helloworld/7753273](https://d2.naver.com/helloworld/7753273)